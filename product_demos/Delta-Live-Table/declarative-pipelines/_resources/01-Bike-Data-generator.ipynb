{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26590a22-fe2f-4172-889f-2ca93c64eabc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "volume_path = \"/Volumes/main/jesse_young_dlt_demo/raw_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "454f7065-cd37-4abd-b79a-cef7b9b0e1d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType, BooleanType\n",
    "\n",
    "# Bike Ride Logs (Raw GPS/Usage Data)\n",
    "ride_log_schema = StructType([\n",
    "    StructField(\"ride_id\", StringType()),  # Unique ride identifier\n",
    "    StructField(\"start_time\", TimestampType()),  # Ride start timestamp\n",
    "    StructField(\"end_time\", TimestampType()),  # Ride end timestamp\n",
    "    StructField(\"start_station_id\", StringType()),\n",
    "    StructField(\"end_station_id\", StringType()),\n",
    "    StructField(\"bike_id\", StringType()),  # Bike identifier\n",
    "    StructField(\"user_type\", StringType()),  # e.g., \"member\", \"casual\"\n",
    "])\n",
    "\n",
    "# Bike Maintenance Logs (Raw Repair Data)\n",
    "maintenance_log_schema = StructType([\n",
    "    StructField(\"maintenance_id\", StringType()),\n",
    "    StructField(\"bike_id\", StringType()),\n",
    "    StructField(\"issue_description\", StringType()),  # e.g., \"flat tire\"\n",
    "    StructField(\"reported_time\", TimestampType()),\n",
    "    StructField(\"resolved_time\", TimestampType()),\n",
    "])\n",
    "\n",
    "# Weather Data\n",
    "weather_schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"temperature_f\", FloatType()),\n",
    "    StructField(\"rainfall_in\", FloatType()),\n",
    "    StructField(\"wind_speed_mph\", FloatType())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3dfd2ae-9b9f-406d-a1bc-051d0dc88493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import random\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "days_to_generate = 100\n",
    "max_rides_per_day = 10\n",
    "odds_of_maintenance = 1.0/50.0\n",
    "\n",
    "bikes = [str(uuid.uuid4()) for _ in range(200)]\n",
    "stations = [str(uuid.uuid4()) for _ in range(25)]\n",
    "\n",
    "# Add bikes to fleet over time\n",
    "date_bike_added = {\n",
    "    # Normal distrubution, where mean is 3/4 of the total days, with a standard deviation of 1/10\n",
    "    bike_id: max(0, min(int(round(random.normalvariate(days_to_generate * .75, days_to_generate * .10))), days_to_generate - 1 if days_to_generate > 0 else 0))\n",
    "    # We'll start with 50 bikes\n",
    "    for bike_id in bikes[50:]\n",
    "}\n",
    "\n",
    "\n",
    "rides = []\n",
    "maintenance_logs= []\n",
    "\n",
    "for bike in bikes:\n",
    "  current_station = random.choice(stations)\n",
    "  remaining_maintenance = 0\n",
    "  current_maintenance = None\n",
    "  for day in reversed(range(days_to_generate)):\n",
    "    # Skip day if bike hasn't been added to the fleet yet\n",
    "    if (days_to_generate - day) < date_bike_added.get(bike, 0):\n",
    "      continue\n",
    "    current_date = datetime.now() - timedelta(days=day)\n",
    "    start_time = datetime.combine(current_date, datetime.min.time())\n",
    "    end_time = datetime.combine(current_date, datetime.max.time())\n",
    "\n",
    "    # If there is a maintenance event, skip the day\n",
    "    if remaining_maintenance > 0:\n",
    "      remaining_maintenance -= 1\n",
    "      if remaining_maintenance == 0:\n",
    "        current_maintenance[\"resolved_time\"] = start_time\n",
    "        maintenance_logs.append(current_maintenance)\n",
    "        current_maintenance = None\n",
    "      continue\n",
    "    \n",
    "    # Generate a random number of trips in a day for this bike\n",
    "    trips_in_day = random.randint(1, max_rides_per_day)\n",
    "\n",
    "    # Generate a random list of start/end times for each trip\n",
    "    total_seconds = (end_time - start_time).total_seconds()\n",
    "    ride_times = sorted([random.randint(0, int(total_seconds)) for _ in range(2 * trips_in_day)])\n",
    "    for i in range(0, 2 * trips_in_day, 2):\n",
    "      rides.append({\n",
    "          \"ride_id\": str(uuid.uuid4()),\n",
    "          \"start_time\": start_time + timedelta(seconds=ride_times[i]),\n",
    "          \"end_time\": start_time + timedelta(seconds=ride_times[i+1]),\n",
    "          \"start_station_id\": current_station,\n",
    "          \"end_station_id\": random.choice(stations),\n",
    "          \"bike_id\": bike,\n",
    "          \"user_type\": random.choice([\"member\", \"non-member\"])\n",
    "      })\n",
    "\n",
    "      # Random odds of a maintenance event\n",
    "      if random.random() < odds_of_maintenance:\n",
    "        current_maintenance = {\n",
    "          \"maintenance_id\": str(uuid.uuid4()),\n",
    "          \"bike_id\": bike,\n",
    "          \"issue_description\": random.choice([\"brakes\", \"chain\", \"tires\", \"seat\", \"handlebars\", \"safety reflectors\"]),\n",
    "          \"reported_time\": start_time + timedelta(seconds=ride_times[i+1])\n",
    "        }\n",
    "        remaining_maintenance = random.randint(1, 5)\n",
    "        break\n",
    "    \n",
    "      \n",
    "\n",
    "# San Francisco monthly averages (index 0=Jan, 11=Dec)\n",
    "monthly_avg = {\n",
    "    \"temp_f\": [57, 60, 62, 63, 64, 67, 67, 68, 70, 70, 64, 58],  # Avg highs\n",
    "    \"rain_in\": [4.72, 1.35, 2.58, 1.35, 0.48, 0.14, 0.01, 0.04, 0.08, 0.94, 3.0, 3.5],\n",
    "    \"wind_mph\": [16, 21, 24, 24, 23, 22, 19, 14, 15, 17, 18, 20],\n",
    "}\n",
    "\n",
    "weather_data = []\n",
    "for day in reversed(range(days_to_generate)):\n",
    "    current_date = datetime.combine(datetime.now() - timedelta(days=day), datetime.min.time())\n",
    "    month_idx = current_date.month - 1  # 0-based index\n",
    "\n",
    "    # Temperature with daily variation and seasonal trend\n",
    "    temp = random.gauss(monthly_avg[\"temp_f\"][month_idx], 3)\n",
    "    temp += 5 * (1 + random.random())  # Add daily variation\n",
    "\n",
    "    # Rainfall with monthly base and chance of showers\n",
    "    rain = max(0, random.gauss(monthly_avg[\"rain_in\"][month_idx] / 30, 0.05))\n",
    "    if random.random() < 0.3:  # 30% chance of no rain even if avg >0\n",
    "        rain = 0.0\n",
    "\n",
    "    # Wind speed with daily variation\n",
    "    wind = max(5, random.gauss(monthly_avg[\"wind_mph\"][month_idx], 4))\n",
    "\n",
    "    weather_data.append(\n",
    "        {\n",
    "            \"timestamp\": current_date,\n",
    "            \"temperature_f\": round(float(temp), 1),\n",
    "            \"rainfall_in\": round(float(rain), 2),\n",
    "            \"wind_speed_mph\": round(float(wind), 1),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "rides_df = spark.createDataFrame(rides, ride_log_schema)\n",
    "maintenance_logs_df = spark.createDataFrame(maintenance_logs, maintenance_log_schema)\n",
    "weather_data_df = spark.createDataFrame(weather_data, weather_schema)\n",
    "\n",
    "\n",
    "rides_df.display()\n",
    "maintenance_logs_df.display()\n",
    "weather_data_df.display()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9972fc4f-10af-4e25-a422-925a1e7b8f34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "maintenance_logs_with_descriptions = spark.sql(\n",
    "    \"\"\"\n",
    "select\n",
    "  * except (issue_description),\n",
    "  case \n",
    "    when rand() > 0.95 then \"Broken\" -- Inject some random bad data\n",
    "    when rand() > 0.95 then null\n",
    "    else ai_query(\n",
    "      \"databricks-meta-llama-3-3-70b-instruct\",\n",
    "      \"You are a user of a bicycle rental service that rents bikes by the hour. The bike you just rented has an issue with the \" || issue_description || \" . You are writing a report of the issue with your bike. Your response should be 2 to 3 sentences. The response should be informal and terse. Your response should not include the bike number.\",\n",
    "      modelParameters => named_struct(\"temperature\", 2, \"top_k\", 100)\n",
    "  ) end as issue_description\n",
    "from\n",
    "  {maintenance_logs}\n",
    "\"\"\",\n",
    "    maintenance_logs=maintenance_logs_df,\n",
    ")\n",
    "\n",
    "maintenance_logs_with_descriptions.limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f853bc1-9732-4df8-b6cd-96b0a3e94a50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "import pandas as pd\n",
    "\n",
    "spark.sql(\"create volume if not exists main.jesse_young_dlt_demo.raw_data\")\n",
    "dbutils.fs.rm(f\"{volume_path}/rides\", recurse=True)\n",
    "dbutils.fs.mkdirs(f\"{volume_path}/rides\")\n",
    "dbutils.fs.rm(f\"{volume_path}/maintenance_logs\", recurse=True)\n",
    "dbutils.fs.mkdirs(f\"{volume_path}/maintenance_logs\")\n",
    "dbutils.fs.rm(f\"{volume_path}/weather\", recurse=True)\n",
    "dbutils.fs.mkdirs(f\"{volume_path}/weather\")\n",
    "\n",
    "\n",
    "def write_to_csv(key: tuple[str], pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    pdf.to_csv(key[0], index=False)\n",
    "    return pd.DataFrame(data={\"file\": [key[0]], \"count\": [pdf.shape[0]]})\n",
    "\n",
    "def write_to_json(key: tuple[str], pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    pdf.to_json(key[0], orient=\"records\")\n",
    "    return pd.DataFrame(data={\"file\": [key[0]], \"count\": [pdf.shape[0]]})\n",
    "\n",
    "rides_df.groupBy(\n",
    "    expr(f\"'{volume_path}/rides/rides_' || date_format(start_time, 'yyyy-MM-dd') || '.csv'\")\n",
    ").applyInPandas(write_to_csv, schema=\"file string, count int\").display()\n",
    "\n",
    "\n",
    "maintenance_logs_with_descriptions.groupBy(\n",
    "    expr(f\"'{volume_path}/maintenance_logs/maintenance_logs_' || date_format(reported_time, 'yyyy-MM-dd') || '.csv'\")\n",
    ").applyInPandas(write_to_csv, schema=\"file string, count int\").display()\n",
    "\n",
    "\n",
    "weather_data_df.groupBy(\n",
    "    expr(f\"'{volume_path}/weather/weather_' || date_format(timestamp, 'yyyy-MM-dd') || '.json'\")\n",
    ").applyInPandas(write_to_json, schema=\"file string, count int\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a40136b-b09a-4481-8797-6b5b8c729695",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "faker"
    ],
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 913238801253704,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "01-Bike-Data-generator",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
