{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf7641be-2c3b-4714-b00a-eb9d661d5241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "# Production Monitoring: Automated Quality at Scale\n",
    "\n",
    "MLflow's production monitoring automatically runs quality assessments on a sample of your production traffic, ensuring your GenAI app maintains high quality standards without manual intervention. MLflow lets you use the same metrics you defined for offline evaluation in production, enabling you to have consistent quality evaluation across your entire application lifecycle - dev to prod.\n",
    "\n",
    "**Key benefits:** \n",
    "\n",
    "- Automated evaluation - Run LLM judges on production traces with configurable sampling rates\n",
    "- Continuous quality assessment - Monitor quality metrics in real-time without disrupting user experience\n",
    "- Cost-effective monitoring - Smart sampling strategies to balance coverage with computational cost\n",
    "\n",
    "Production monitoring enables you to deploy confidently, knowing that you will proactively detect issues so you can address them before they cause a major impact to your users.\n",
    "\n",
    "<img src=\"https://i.imgur.com/wv4p562.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c4d8b9d-70b2-4a46-b7d5-26e6a5c1cddc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.agents.monitoring import (\n",
    "  AssessmentsSuiteConfig,\n",
    "  GuidelinesJudge,\n",
    "  create_external_monitor,\n",
    ")\n",
    "\n",
    "# Create external monitor for automated production monitoring\n",
    "external_monitor = create_external_monitor(\n",
    "  # Change to a Unity Catalog schema where you have CREATE TABLE permissions.\n",
    "  catalog_name=UC_CATALOG,\n",
    "  schema_name=UC_SCHEMA,\n",
    "  assessments_config=AssessmentsSuiteConfig(\n",
    "    sample=1.0,  # sampling rate\n",
    "    assessments=[\n",
    "        # Builtin judges\n",
    "        RelevanceToQuery(),\n",
    "        RetrievalGroundedness(),\n",
    "        RetrievalRelevance(),\n",
    "        Safety(),\n",
    "        # Guidelines can refer to the request and response.\n",
    "        GuidelinesJudge(guidelines={\n",
    "  'accuracy': [\n",
    "    \"\"\"The response correctly references all factual information from the provided_info based on these rules:\n",
    "- All factual information must be directly sourced from the provided data with NO fabrication\n",
    "- Names, dates, numbers, and company details must be 100% accurate with no errors\n",
    "- Meeting discussions must be summarized with the exact same sentiment and priority as presented in the data\n",
    "- Support ticket information must include correct ticket IDs, status, and resolution details when available\n",
    "- All product usage statistics must be presented with the same metrics provided in the data\n",
    "- No references to CloudFlow features, services, or offerings unless specifically mentioned in the customer data\n",
    "- AUTOMATIC FAIL if any information is mentioned that is not explicitly provided in the data\"\"\"\n",
    "  ],\n",
    "  'personalized': [\n",
    "    \"\"\"The response demonstrates clear personalization based on the provided_info based on these rules:\n",
    "- Email must begin by referencing the most recent meeting/interaction\n",
    "- Immediatly next, the email must address the customer's MOST pressing concern as evidenced in the data\n",
    "- Content structure must be customized based on the account's health status (critical issues first for \"Fair\" or \"Poor\" accounts)\n",
    "- Industry-specific language must be used that reflects the customer's sector\n",
    "- Recommendations must ONLY reference features that are:\n",
    "  a) Listed as \"least_used_features\" in the data, AND\n",
    "  b) Directly related to the \"potential_opportunity\" field\n",
    "- Relationship history must be acknowledged (new vs. mature relationship)\n",
    "- Deal stage must influence communication approach (implementation vs. renewal vs. growth)\n",
    "- AUTOMATIC FAIL if recommendations could be copied to another customer in a different situation\"\"\"\n",
    "  ],\n",
    "  'relevance': [\n",
    "    \"\"\"The response prioritizes content that matters to the recipient in the provided_info based on these rules:\n",
    "- Critical support tickets (status=\"Open (Critical)\") must be addressed after the greeting, reference to the most recent interaction, any pleasantrys, and references to closed tickets\n",
    "    - it is ok if they name is slightly different as long as it is clearly the same issue as in the provided_info\n",
    "- Time-sensitive action items must be addressed before general updates\n",
    "- Content must be ordered by descending urgency as defined by:\n",
    "  1. Critical support issues\n",
    "  2. Action items explicitly stated in most recent meeting\n",
    "  3. Upcoming renewal if within 30 days\n",
    "  4. Recently resolved issues\n",
    "  5. Usage trends and recommendations\n",
    "- No more than ONE feature recommendation for accounts with open critical issues\n",
    "- No mentions of company news, product releases, or success stories not directly requested by the customer\n",
    "- No calls to action unrelated to the immediate needs in the data\n",
    "- AUTOMATIC FAIL if the email requests a meeting without being tied to a specific action item or opportunity in the data\"\"\"\n",
    "  ],\n",
    "}),\n",
    "    ],\n",
    "  ),\n",
    ")\n",
    "\n",
    "print(f\"Monitor created: {external_monitor}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bd54aaa-71b8-4082-affb-cf58bfd8ec25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_c75093c8-0895-475e-8c1b-6acacfe3368b",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "05.production-monitoring",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
